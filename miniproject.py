# -*- coding: utf-8 -*-
"""MiniProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/180Ex8sqrjusMXrZU_9z8ShncfVI58IFf
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Step 1: Load and preprocess the dataset
data = pd.read_csv('Occupancy_Estimation.csv')

# Remove irrelevant or redundant columns
data = data[['S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp',
             'S1_Light', 'S2_Light', 'S3_Light', 'S4_Light',
             'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound',
             'S5_CO2', 'S5_CO2_Slope', 'S6_PIR', 'S7_PIR',
             'Room_Occupancy_Count']]

# Handle missing values
data = data.dropna()

# Step 2: Feature selection/engineering
X = data[['S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp',
          'S1_Light', 'S2_Light', 'S3_Light', 'S4_Light',
          'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound',
          'S5_CO2', 'S5_CO2_Slope', 'S6_PIR', 'S7_PIR']]
y = data['Room_Occupancy_Count']

# Step 3: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Normalize or scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 5: Model selection and training
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Step 6: Model evaluation
y_pred = model.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
print(f"Root Mean Squared Error: {rmse}")

# Step 7: Deployment (not shown in the code)
# Once satisfied with the model's performance, deploy it for real-time estimation

import joblib

# Save the trained model
joblib.dump(model, 'model.pkl')

def predict_occupancy(data):
    # Load the trained model
    model = joblib.load('model.pkl')

    # Preprocess the input data
    data_scaled = scaler.transform(data)

    # Make predictions
    predictions = model.predict(data_scaled)
    return predictions

from sklearn.preprocessing import StandardScaler
import joblib

# Assuming X_train is your training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Save the fitted scaler to a file
joblib.dump(scaler, 'scaler.pkl')
